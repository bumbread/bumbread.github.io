<!DOCTYPE html><html><head><meta charset=utf-8><meta name=referrer content=no-referrer><meta name=viewport content="width=device-width, initial-scale=0.7"><meta http-equiv=Permissions-Policy content=interest-cohort=()><link rel=stylesheet href=/styles.css><link rel=favicon href=/favicon.ico><link rel=apple-touch-icon href=/favicon.ico><script src=/coloring.js defer></script><body><h1 id=unicode-in-programming-languages>Unicode in programming languages.</h1><p>It has become a trend in recent years to support unicode in programming languages. Java, Python, JavaScript, Go, and lots of other new languages support unicode in some form. In this article I would like to explore different possibilities in the space of source file encodings, when it comes to programming languages.<p>In the older times, lanugages used to only support the 127(±) ASCII characters. Some languages could handle ANSI extensions in the source files. ANSI pages are problematic with respect to portability and internationalization.<p>The first problem is that you weren't able to use more than one ANSI codepage at a time. If you wanted to make an app that lets greek people talk to people from india, well, the only thing I can do is to wish you good luck. It is be pretty much impossible to handle characters from more than two non-english languages in one string, without breaking something.<p>The second problem is that codepages have to be switched, either manually or by software, and sometimes developers would forget about switching the codepage to the one their app uses and just publish their software assuming a particular codepage, without forcing operating system to switch onto that page. Even if the language is the same, it doesn't mean that the codepage will be. For example, russian language had at least two codepages: an older one codepage 866, used primarily in the DOS era, and a newer one Windows 1251.<p><a href=https://en.wikipedia.org/wiki/Mojibake>Mojibake</a> was a common phenomenon in that time. It happened mostly when software targeted and displaying one codepage was run on an operating system with a different codepage set.<p>Unicode changed the game. It was a single encoding for every language on earth (and more). Although not entirely eliminating the problems with encoding (because some old software still uses codepages!), people are slowly moving towards supporting it everywhere.<p>Major operating systems support unicode. Now it is possible to write a single program that, when tries to display a string in a given language, will display the same string on any other computer! More than that, you can mix characters from any language in the world in one string. This makes portability problem much smaller.<p>But with a few good game-changers, unicode also brought a few problems. In particular, handling unicode is a little bit harder. Taking input from the user in a sensible manner is <a href=https://lord.io/text-editing-hates-you-too/>almost impossible</a>. 100% correct rendering of unicode is an utter hell which one should not try to step their foot into.<p>I would like to explore pros and cons of supporting various unicode features in a programming language, how it can be done in different ways and which tradeoffs it will present to you, as a programming language developer. I will try to be unbiased as much as possible. I have fact-checked most of the statements written in this article. I will try to avoid personal reflection (I'm not saying there isn't any!). You should note that <em>there is no general "correct" solution</em>. Pick one for yourself, I shall only provide you some food for thought.<p>In this article I will be using the word "european" to mean "european except english", and "english" to refer to both UK and US variants of the language (since they use the same characters). If I say asian, this specifically refers to the languages that support CJK character set.<p>Also to clear misunderstandings I have preserved the encoding of this document in a believable manner. If you want to copy a character, you can be sure that it's code point is the same as I said it is.<p>Before we start talking I would like to introduce you to the term <em>unicode</em> and how the <em>UTF</em> encodings work. This will be important in our conversation. If you already know how UTF-8, UTF-16 and UTF-32 characters are encoded, then you can skip the following section.<h2 id=unicode>Unicode</h2><p>Unicode is an international <em>standard</em> for character encoding. I want to make an emphasis on the fact that Unicode is <em>not an encoding</em>, it's <em>a standard</em>. Microsoft uses poor terminology by associating the term Unicode with UCS-2 encoded strings. This sometimes makes communication harder.<p>The standard of unicode assigns every "character" a unique codepoint. The "character" doesn't have to be a graphical one, like we are used to. It can be anything, if you're familiar with ASCII you know that some of the things we call characters include <em>line terminators</em>, <em>carriage returns</em>, <em>space characters</em>. Unicode has more than that, besides these control characters it also has <em>diacritics</em>, <em>right-to-left marker</em> that is used for arabic text, <em>byte-order masks</em> and much more. This is what makes correct unicode rendering hard and almost impossible. Sometimes it just doesn't bend our intuition.<p>Every character in unicode is assigned a unique number. The minimum number is <code>0</code>. The maximum is <code>0x10FFFF</code>. And this, I will explain later why, but no character is assigned a value in between <code>0xD800</code> through <code>0xDFFF</code> (inclusive). So every unicode character has a value either from <code>0</code> to <code>0xD7FF</code> or from <code>0xE000</code> to <code>0x10FFFF</code> (all inclusive).<p>The encodings of unicode are pretty straightforward. There are three major unicode encodings: UTF-8, UTF-16 and UTF-32. In each one of these encodings a code point is encoded via a series of <em>code units</em>.<p>UTF32 is the simplest one of the three. Since all code units are less than <code>0x110000</code> which are less than <code>0x80000000</code>, each code point can be trivially stored as an integer in an array. You can think of it as the plain old ASCII strings, but instead of 1 byte per character you get 4 bytes per character.<p>Encoding the letter <code>h</code> in UTF-32 will produce 4 bytes: <code>[0x68] [0x00] [0x00] [0x00]</code>. Note that the low-order byte comes first. It actually can be last sometimes. Also note that compared to ASCII, we now have three unused bytes. For english letters UTF-32 implies 75% space loss.<p>UTF-16 is an encoding that is the most optimal in memory for most european and asian languages. In UTF-16 every code point is encoded by either two bytes or by four bytes. Each <em>code unit</em> uses 16 bits. However due to variativity in its encoding length, you can not in general access a character by it's index. Looping over characters in a UTF-16 string may be a challenge.<p>When UTF-16 code point is encoded using a single code unit, that unit is stored directly as a 16-bit integer. When UTF-16 code point is encoded using two code units, one of them is called <em>high surrogate</em> and the other is called <em>a low surrogate</em>. Surrogates belong to the range <code>0xD800</code>..<code>0xDFFF</code>. That's why no single character is encoded using these codepoints in that range.<p>For more information about encoding UTF-16 you can use this wikipedia page: <a href=https://en.wikipedia.org/wiki/UTF-16>Encoding UTF-16</a>. That information will be too much for a single article. For now just note that english, european and most asian characters will take up 2 bytes, whereas emoji and other asian characters will take up 4 bytes. (I'm not counting composite emoji made out of "several characters").<p>UTF-8 provides the best memory tradeoff for ASCII-heavy applications, but if at the same time you want to use other lanugages. Each code point is encoded using either 1 byte, 2 byte, 3 bytes or 4 bytes. Note that UTF-8 is <em>fully ASCII-compatible</em>. An application that expects ASCII string will be able to process a UTF-8 string.<p>The reason it works is because every ASCII character starts with <code>0b0...</code>. The top-most bit is always zero. If you want to have a 2-byte sequence, you start the first byte with <code>0b110..</code>, and the next byte would start with <code>0b10..</code> (neither of the two starts from zero).<p>If you want a 3-byte sequence you start the first byte with <code>0b1110..</code> and it is followed by two continuation bytes that start with <code>0b10..</code>.<p>If you want a 4-byte sequence, you start the first byte with <code>0b11110..</code>, and this is followed by 3 continuation bytes that start with <code>0b10..</code>.<p>In either case there is an important observation: you can easily tell apart a continuation byte from starting byte. You can easily pick a byte in a string at random and scan back until the first non-continuation byte. This may be useful sometimes.<p>Interesting fact -- utf-8 <em>could</em> support up to 6-byte sequences. However that would not be compatible with UTF-16 and therefore no code point is ever encoded using 5 or more -byte sequence.<h2 id=strings>Strings</h2><p>The first paragraphs of the article were devoted to a problem of strings in programming languages and programs. Unless you are making a language for very specific purpose I wouldn't recommend removing/not adding support for unicode strings. The strings are the parts of the program that can be displayed to a user, and they don't <em>have</em> to be able to know English to communicate to your program. Also adding a UTF-8 parser onto the bottom of a lexer is a trivial task.<p>Various operating system and libraries support different formats of unicode encodings. As far as I'm aware Linux kernel takes in UTF-8 strings, because they are easy to scan through in an ASCII-compatible way. Windows uses UCS-2, but Windows 10 versions have support for UTF-8 codepage <code>65001</code>. So using UTF-8 everywhere might be a sane bet, if you don't want to introduce several types of strings into your language or if you don't want to slow down your libraries by converting formats when talking to the OS.<p>The tradeoff is that UTF-8 strings aren't trivial to scan through. A few library calls like <code>decode_utf8_rune_at</code> may be useful, but when compared to using simple O(1) accessor (viz. <code>str[i]</code>), it is harder. By contrast, UTF-16 supports almost every european character and most of the CJK (Chinese-Japanese-Korean) characters. UTF-16 doesn't cover emoji's so if further support is required, you may want to consider using UTF-32.<p>Note that, however UTF-32 adds a significant memory overhead. +75% overhead for english, +50% for european characters and most CJK characters. UTF-16 is a perfect balance for most of the european languages. UTF-8 provides +25% overhead for european characters, but perfect for applications that extensively use english and need ASCII compatibility.<p>Your language may be aimed at a more precise control over strings, so it might be useful to define different literals for strings in various UTF formats. viz.:<pre><code class="ai language-ai">let utf8string = u8"строка";
let utf16string = u16"文字列"
let utf32string = u32"waste of space";
</code></pre><p>We should draw a clear line between <em>source encoding</em> and <em>string encoding</em>. If your program source code is encoded via UTF-8 it doesn't imply that the string literals in the program of the user are also UTF-8. At least this shouldn't be the implication. The compiler should re-encode the strings into a particular format when such literals are discovered.<p>Although it is technically possible to add a different category of strings, that will be taken <em>literally</em>, i.e. byte-by-byte from the source encoding. Please don't do it, its very hacky and may cause more issues than it solves.<h2 id=character-literals>Character literals</h2><p>I want you to look at the following piece of code (which is located in a statement context) and tell me which of these two lines will compile, if any:<pre><code>var b = 'だ';
var c = 'だ';
</code></pre><p>Even if both lines look identical to you, they are, in fact, different. In the first line we have <em>japanese "da" hiragana</em>. In the second line we have <em>japanese "ta" hiragana</em> followed by <em>japanese dakuten mark</em> (a diacritic that makes the consonant sound "soft").<p>So since Go does not normalize unicode strings, the second string, in Go compiler's opinion contains two characters. Character literals in Go are required to have a single character inside, so the compiler reports an error at the second line.<p>In practice though, this does not matter. IME (Input method editors), which are used by japanese people typically normalize the unicode strings. Therefore even if a japanese person types in <em>hiragana ta</em> followed by <em>dakuten mark</em>, the outcome in the file will be the same (I had checked this using default japanese IME on Windows 10, using kana input).<p>On the other hand, unicode normalization procedure may be introduce a small performance tradeoff for compilers. Different kinds of normalization performed may also change the results in different ways. For example, if normalization produces a <em>decomposed string</em>, meaning that a single character (viz. <code>だ</code>) becomes a sequence of character and diacritics in a certain order (viz. <code>た</code> + dakuten). Both of the lines in the example above would not compile.<p>If, however, you normalize by <em>composing</em> a string, you may end up with both of the lines above compiling.<h2 id=variables>Variables</h2><p>String literals and character literals have pretty trivial problems, compared to unicode support in identifiers.<h3 id=1-disabling-unicode-identifiers>1. Disabling unicode identifiers</h3><p>Not supporting unicode identifiers is not an excuse for not doing unicode parsing. It matters during the error handling stage. Consider this situation with a word <code>dеf</code>, where <code>e</code> is a cyrillic character that deceivingly looks like an english 'e' letter.<pre><code class="ai language-ai">dРµf
 ^ 'Р' invalid character
def
 ^ 'е' invalid character
dеf
 ^ 'u+D0B5' invalid character
</code></pre><p>The first error message is what happens if you assume that not supporting unicode implies that the input will not be a unicode string, and you just handle the ASCII. The first byte of UTF-8 sequence for cyrillic <code>е</code> character is <code>D0</code>. In codepage 1251, the most common encoding used in russia that character is <code>Р</code> (cyrillic R).<p>You can actually assume ASCII, but on windows make sure that your user has codepage <code>65001</code> enabled. In that case your error message becomes like the second one above. This is not helpful at all! It looks like compiler is disagreeing with the specification of the language itself or just randomly started trolling you.<p>That's why when you print error messages it is better to, instead of printing characters in the source encoding print them as unicode codepoints.<p>Therefore if you're going to <strong>disable</strong> support for unicode you have to do some <strong>utf-8 parsing</strong>, at least for the error messages.<h3 id=2-rationale-for-supportingnot-supporting-the-unicode-identifiers>2. Rationale for supporting/not supporting the unicode identifiers</h3><p>Why would anyone care about the unicode identifiers? Other countries, of course. If your language is meant to be used to teach at foreign schools or other institutions, or to be used by non-professional individuals, then unicode identifiers may be what you want to have a support on in your language. It makes teaching programming easier and more accessible, providing a better opportunity for students to follow the flow of an algorithm.<p>If you disable the unicode support in identifiers, however, a lot of people will be able to come up with english transliterations. In practice, that is what's happening for a lot of languages that do not use Unicode identifiers. Even if you disable identifiers for that particular reason there's no way that will not force people to use transliterations/romanizations.<pre><code class="ai language-ai">gakusei.denwaBangou = 1234311221;
</code></pre><p>There is zero chance that unicode support in identifiers will do much harm to the ecosystem of the language, that the libraries and tools written in and for that language. Every foreign school/university/tutorials teaches future programmers that they should always use ASCII characters for their identifiers. Most of the universities include an additional english course, so understanding, at least a primitive one, is also not of much concern. Self-taught programmers for the most part know english. As a rule, they generally also self-taught in english.<p>People not involved in programming for anything other than personal projects are unlikely to be making libraries and stuff. You can look at any languages that do support unicode identifiers, and the worst their ecosystem has is a line that looks kind of like this:<pre><code class="ai language-ai">let π = 3.141592;
</code></pre><p>Which is not terrible, assuming you have a <code>pi</code> constant defined nearby.<p>People usually don't care that they can define <code>pi</code> as a single greek letter in their library: only greek people will be able to use that library!<p>Including unicode identifiers, however, makes one particular annoyance with your language possible. Have you ever spend an hour trying to fix a <em>syntax error</em>? Well consider following Python code and tell me what is the error:<pre><code>dеf func(x):
    ^^^^  
    unexpected identifier

  return x+1;
</code></pre><p>This a real error. I had verified this particular error message using python 3 interpreter on windows.<p>Python thinks that <code>dеf</code> is an identifier (it is), but identfiers can not be followed by other identifiers directly. The core of the error is that the <code>е</code> letter is a cyrillic one, not ASCII letter.<p>As a russian person, let me tell you, it's the worst kind of bug! I can say this for two reasons: first, it is a rare bug. You never expect this particular syntax error to occur. Which means that wrong keyboard layout would be the last thing you'll try.<p>Secondly, it is real. Having wrong keyboard layout usually implies that the characters you type will be wrong. What you think you type would usually be different from what you see what you type. It is not the case with cyrillic <code>с</code>. On russian keyboards <code>с</code> is located at the same key as the english <code>c</code>. This is made worse by a particular guarding move people do against wrong keyboard layout errors: type a character, see if it's the one you wanted to type, and if not, erase all the characters that didn't match.<p>You may want consider forbidding mixing up english and other foreign characters in the same identifier. Treat that as a lexing error. This can make the error message a bit nicer:<pre><code>dеf func(x):
 ^ unexpected '\u+0435': mixing national and english alphabets.

  return x+1;
</code></pre><h3 id=3-which-characters-to-enable>3. Which characters to enable</h3><p>If you want to use unicode in identifiers, you shouldn't be light about making the decision about which characters are allowed. If you misdefine the character set used for identifiers, you may end up with some "smart person" making unreadable mess just for fun. Or worse: person who genuinely wants to input a word would be unable because "diacritics are not allowed".<p>For these purposes unicode defines a <a href=http://unicode.org/reports/tr31>special standard</a> relating to naming identifiers in programming languages. You may find particular information there.<h2 id=links>Links</h2><ol><li><a href=https://unicode.org/reports/tr15/>Unicode standard annex: unicode normalization</a><li><a href=http://unicode.org/reports/tr31>Unicode standard annex: unicode identifier syntax</a><li><a href=https://rust-lang.github.io/rfcs/2457-non-ascii-idents.html>Rust RFC: non-ascii identifiers</a><li><a href=https://github.com/ziglang/zig/issues/663>Zig issues: source encoding</a><li><a href=https://github.com/ziglang/zig/issues/3947>Zig issues: unicode identifiers</a><li><a href=https://en.wikipedia.org/wiki/UTF-8>UTF-8 on wikipedia</a><li><a href=https://en.wikipedia.org/wiki/UTF-16>UTF-16 on wikipedia</a></ol>